<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Chris</title><link>http://kjczarne.github.io/papers/</link><description>Recent content on Chris</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&amp;copy; 2024 Chris J. Czarnecki. Fair use permitted with authorship attribution. Commercial use disallowed without explicit permission. Some links in blog articles are affiliate links when noted.</copyright><lastBuildDate>Mon, 05 Aug 2024 20:32:50 -0400</lastBuildDate><atom:link href="http://kjczarne.github.io/papers/index.xml" rel="self" type="application/rss+xml"/><item><title>Plate Ellipse Fitting</title><link>http://kjczarne.github.io/papers/plate-ellipse-fitting/</link><pubDate>Mon, 05 Aug 2024 20:32:50 -0400</pubDate><guid>http://kjczarne.github.io/papers/plate-ellipse-fitting/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Ellipse estimation is an important topic in food image processing because it can be leveraged to parameterize plates and bowls, which in turn can be used to estimate camera view angles and food portion sizes. Automatically detecting the elliptical rim of plates and bowls and estimating their ellipse parameters for data &amp;ldquo;in-the-wild&amp;rdquo; is challenging: diverse camera angles and plate shapes could have been used for capture, noisy background, multiple non-uniform plates and bowls in the image could be present. Recent advancements in foundational models offer promising capabilities for zero-shot semantic understanding and object segmentation. However, the output mask boundaries for plates and bowls generated by these models often lack consistency and precision compared to traditional ellipse fitting methods. In this paper, we combine ellipse fitting with semantic information extracted by zero-shot foundational models and propose WildEllipseFit, a method to detect and estimate the elliptical rim for plate and bowl. Evaluation on the proposed Yummly-ellipse dataset demonstrates its efficacy and zero-shot capability in real-world scenarios.&lt;/p></description></item><item><title>How Much You Ate</title><link>http://kjczarne.github.io/papers/how-much-you-ate/</link><pubDate>Sat, 03 Aug 2024 14:25:47 -0400</pubDate><guid>http://kjczarne.github.io/papers/how-much-you-ate/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Monitoring dietary intake is a crucial aspect of promoting healthy living. In recent years, advances in computer vision technology have facilitated dietary intake monitoring through the use of images and depth cameras. However, the current state-of-the-art image-based food portion estimation algorithms assume that users take images of their meals one or two times, which can be inconvenient and fail to capture food items that are not visible from a top-down perspective, such as ingredients submerged in a stew. To address these limitations, we introduce an innovative solution that utilizes stationary user-facing cameras to track food items on utensils, not requiring any change of camera perspective after installation. The shallow depth of utensils provides a more favorable angle for capturing food items, and tracking them on the utensil&amp;rsquo;s surface offers a significantly more accurate estimation of dietary intake without the need for post-meal image capture. The system is reliable for estimation of nutritional content of liquid-solid heterogeneous mixtures such as soups and stews. Through a series of experiments, we demonstrate the exceptional potential of our method as a non-invasive, user-friendly, and highly accurate dietary intake monitoring tool.&lt;/p></description></item></channel></rss>